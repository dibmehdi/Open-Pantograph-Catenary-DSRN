# -*- coding: utf-8 -*-
"""Decrement

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GrCmAzkeaH9KloExmhTTIN_yUZ1nJkyY
"""

import cv2
import time
from tqdm import tqdm

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

input_video_path = '/content/drive/MyDrive/_edit/12 AL_combined_part1.mp4'
output_video_path = '/content/drive/MyDrive/_edit/Thenia.mp4'


# OpenCV video capture from Google Drive
cap = cv2.VideoCapture(input_video_path)
frame_width = int(cap.get(3))
frame_height = int(cap.get(4))
input_fps = int(cap.get(cv2.CAP_PROP_FPS))
output_fps = 24
total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

# Define the codec and create VideoWriter object
fourcc = cv2.VideoWriter_fourcc(*'XVID')
out = cv2.VideoWriter(output_video_path, fourcc, output_fps, (frame_width, frame_height))

# Starting and ending points for the video and incrementing number
start_point = 9.242
end_point = 68.27
reference_point_time = 23 * 60 + 34  # Minute 18 and 18 seconds
reference_point = 53.27


# Read the first frame to get dimensions
ret, prev_frame = cap.read()
height, width, _ = prev_frame.shape

# Variables for timing and incrementing
frame_duration = 1 / input_fps

# Process video frames
for frame_idx in tqdm(range(total_frames), desc="Processing Frames"):
    ret, current_frame = cap.read()
    if not ret:
        break

    # Convert frames to grayscale
    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
    current_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)

    # Calculate frame difference
    frame_diff = cv2.absdiff(current_gray, prev_gray)
    diff_intensity = cv2.sumElems(frame_diff)[0] / (frame_diff.shape[0] * frame_diff.shape[1])

    # Calculate the current point based on video progress
    elapsed_time = frame_idx / input_fps
    if elapsed_time < reference_point_time:
        # Interpolate between start_point and reference_point
        if start_point > end_point:
            current_point = start_point - (start_point - reference_point) * (elapsed_time / reference_point_time)
        else:
            current_point = start_point + (reference_point - start_point) * (elapsed_time / reference_point_time)
    else:
        # Interpolate between reference_point and end_point
        if start_point > end_point:
            current_point = reference_point - (reference_point - end_point) * ((elapsed_time - reference_point_time) / (total_frames / input_fps - reference_point_time))
            current_point = max(current_point, end_point)
        else:
            current_point = reference_point + (end_point - reference_point) * ((elapsed_time - reference_point_time) / (total_frames / input_fps - reference_point_time))
            current_point = min(current_point, end_point)

    # Convert current_point to formatted string (e.g., "00.114")
    current_point_str = f"{current_point:.3f}"

    # Overlay the current point on the frame
    cv2.putText(current_frame, f"PK {current_point_str}", (10, height - 20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)

    # Write the frame to the output video
    out.write(current_frame)

    # Update the previous frame
    prev_frame = current_frame

# Release the video capture, VideoWriter
cap.release()
out.release()
cv2.destroyAllWindows()

import cv2
import time
from tqdm import tqdm

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Video paths on Google Drive
input_video_path = '/content/drive/MyDrive/Combined_Video/12 RT_combined.mp4'
output_video_path = '/content/drive/MyDrive/Combined_Video/12_RT_combined_with_overlay. Thenia.mp4'

# OpenCV video capture from Google Drive
cap = cv2.VideoCapture(input_video_path)
frame_width = int(cap.get(3))
frame_height = int(cap.get(4))
input_fps = int(cap.get(cv2.CAP_PROP_FPS))
output_fps = 24
total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

# Define the codec and create VideoWriter object
fourcc = cv2.VideoWriter_fourcc(*'XVID')
out = cv2.VideoWriter(output_video_path, fourcc, output_fps, (frame_width, frame_height))


start_point = 63.135 # Updated start point
end_point = 0.114
reference_point_time = 18 * 60 + 18 # Minute 18 and 18 seconds
reference_point = 52.27

# Read the first frame to get dimensions
ret, prev_frame = cap.read()
height, width, _ = prev_frame.shape

# Variables for timing and incrementing
frame_duration = 1 / input_fps
prev_diff_intensity = None
current_point = None

# Process video frames
for frame_idx in tqdm(range(total_frames), desc="Processing Frames"):
    ret, current_frame = cap.read()
    if not ret:
        break

    # Convert frames to grayscale
    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
    current_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)

    # Calculate frame difference
    frame_diff = cv2.absdiff(current_gray, prev_gray)
    diff_intensity = cv2.sumElems(frame_diff)[0] / (frame_diff.shape[0] * frame_diff.shape[1])

    # Suppress incrementation if frame difference is below a threshold
    if prev_diff_intensity is not None and diff_intensity < 5:  # Adjust threshold as needed
        if current_point is None:
            current_point = reference_point
    else:
        # Calculate the current point based on video progress
        elapsed_time = frame_idx / input_fps
        if elapsed_time < reference_point_time:
            # Interpolate between start_point and reference_point
            if start_point > end_point:
                current_point = start_point - (start_point - reference_point) * (elapsed_time / reference_point_time)
            else:
                current_point = start_point + (reference_point - start_point) * (elapsed_time / reference_point_time)
        else:
            # Interpolate between reference_point and end_point
            if start_point > end_point:
                current_point = reference_point - (reference_point - end_point) * ((elapsed_time - reference_point_time) / (total_frames / input_fps - reference_point_time))
                current_point = max(current_point, end_point)
            else:
                current_point = reference_point + (end_point - reference_point) * ((elapsed_time - reference_point_time) / (total_frames / input_fps - reference_point_time))
                current_point = min(current_point, end_point)

    # Convert current_point to formatted string (e.g., "00.114")
    current_point_str = f"{current_point:.3f}"

    # Overlay the current point on the frame
    cv2.putText(current_frame, f"PK {current_point_str}", (10, height - 20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)

    # Write the frame to the output video
    out.write(current_frame)

    # Update the previous frame and diff_intensity
    prev_frame = current_frame
    prev_diff_intensity = diff_intensity

# Release the video capture, VideoWriter
cap.release()
out.release()
cv2.destroyAllWindows()

"""# Code Without Reference ***point***"""

import cv2
import time
from tqdm import tqdm

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Video paths on Google Drive
input_video_path = '/content/drive/MyDrive/_edit/14 RT_combined_part2.mp4'
output_video_path = '/content/drive/MyDrive/_edit/Birtouta_Zeralda.mp4'

# OpenCV video capture from Google Drive
cap = cv2.VideoCapture(input_video_path)
frame_width = int(cap.get(3))
frame_height = int(cap.get(4))
input_fps = int(cap.get(cv2.CAP_PROP_FPS))
output_fps = 24
total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

# Define the codec and create VideoWriter object
fourcc = cv2.VideoWriter_fourcc(*'XVID')
out = cv2.VideoWriter(output_video_path, fourcc, output_fps, (frame_width, frame_height))

# Starting and ending points for the video and incrementing number
start_point = 25.488
end_point = 20.10

# Read the first frame to get dimensions
ret, prev_frame = cap.read()
height, width, _ = prev_frame.shape

# Variables for timing and incrementing
frame_duration = 1 / input_fps

# Process video frames
for frame_idx in tqdm(range(total_frames), desc="Processing Frames"):
    ret, current_frame = cap.read()
    if not ret:
        break

    # Calculate the current point based on video progress
    elapsed_time = frame_idx / input_fps
    current_point = start_point + (end_point - start_point) * (elapsed_time / (total_frames / input_fps))
    current_point = min(current_point, end_point)

    # Convert current_point to formatted string (e.g., "00.114")
    current_point_str = f"{current_point:.3f}"

    # Overlay the current point on the frame
    cv2.putText(current_frame, f"PK {current_point_str}", (10, height - 20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)

    # Write the frame to the output video
    out.write(current_frame)

    # Update the previous frame
    prev_frame = current_frame

# Release the video capture, VideoWriter
cap.release()
out.release()
cv2.destroyAllWindows()

"""# ***Many Reference Points***"""

import cv2
import time
from tqdm import tqdm

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Video paths on Google Drive
input_video_path = '/content/drive/MyDrive/Combined_Video/14 RT_combined.mp4'
output_video_path = '/content/drive/MyDrive/Combined_Video/14_RT_combined_with_overlay El Affroun.mp4'

# OpenCV video capture from Google Drive
cap = cv2.VideoCapture(input_video_path)
frame_width = int(cap.get(3))
frame_height = int(cap.get(4))
input_fps = int(cap.get(cv2.CAP_PROP_FPS))
output_fps = 24
total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

# Define the codec and create VideoWriter object
fourcc = cv2.VideoWriter_fourcc(*'XVID')
out = cv2.VideoWriter(output_video_path, fourcc, output_fps, (frame_width, frame_height))


# Starting and ending points for the video and incrementing number
start_point = 68.376
end_point = 1.39

# Reference points and their corresponding times and texts
reference_points = [(13 * 60 + 25, 25.488, "Gare Birtouta"), (17 * 60 + 56, 20.10, "Zeralda")]  # [(seconds, PK, text), ...]

# Read the first frame to get dimensions
ret, prev_frame = cap.read()
height, width, _ = prev_frame.shape

# Variables for timing and incrementing
frame_duration = 1 / input_fps
prev_diff_intensity = None
current_point = None

# Process video frames
for frame_idx in tqdm(range(total_frames), desc="Processing Frames"):
    ret, current_frame = cap.read()
    if not ret:
        break

    # Convert frames to grayscale
    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
    current_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)

    # Calculate frame difference
    frame_diff = cv2.absdiff(current_gray, prev_gray)
    diff_intensity = cv2.sumElems(frame_diff)[0] / (frame_diff.shape[0] * frame_diff.shape[1])

    # Suppress incrementation if frame difference is below a threshold
    if prev_diff_intensity is not None and diff_intensity < 5:  # Adjust threshold as needed
        if current_point is None:
            current_point = start_point
    else:
        # Calculate the current point based on video progress
        elapsed_time = frame_idx / input_fps

        # Interpolate between the reference points
        if elapsed_time < reference_points[0][0]:
            # Interpolate between start_point and reference1
            t = elapsed_time / reference_points[0][0]
            current_point = start_point + (reference_points[0][1] - start_point) * t
        elif elapsed_time >= reference_points[-1][0]:
            # Interpolate between reference2 and end_point
            t = (elapsed_time - reference_points[-1][0]) / (total_frames / input_fps - reference_points[-1][0])
            current_point = reference_points[-1][1] + (end_point - reference_points[-1][1]) * t
        else:
            # Find the reference points that bound the current time
            prev_reference_time, prev_reference_pk, prev_reference_text = reference_points[0]
            next_reference_time, next_reference_pk, next_reference_text = reference_points[-1]

            for ref_time, ref_pk, ref_text in reference_points:
                if ref_time <= elapsed_time:
                    prev_reference_time, prev_reference_pk, prev_reference_text = ref_time, ref_pk, ref_text
                else:
                    next_reference_time, next_reference_pk, next_reference_text = ref_time, ref_pk, ref_text
                    break

            # Interpolate between the two reference points
            t = (elapsed_time - prev_reference_time) / (next_reference_time - prev_reference_time)
            current_point = prev_reference_pk + (next_reference_pk - prev_reference_pk) * t
            current_text = prev_reference_text

    # Convert current_point to formatted string (e.g., "00.114")
    current_point_str = f"{current_point:.3f}"

    # Overlay the current point on the frame
    cv2.putText(current_frame, f"PK {current_point_str}", (10, height - 20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)

    # Check if the current time matches a reference point and overlay the text
    for ref_time, ref_pk, ref_text in reference_points:
        if abs(elapsed_time - ref_time) < 1 / input_fps:  # Adjust tolerance as needed
            cv2.putText(current_frame, ref_text, (10, height - 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv2.LINE_AA)
  # Write the frame to the output video
    out.write(current_frame)

    # Update the previous frame and diff_intensity
    prev_frame = current_frame
    prev_diff_intensity = diff_intensity

# Release the video capture, VideoWriter
cap.release()
out.release()
cv2.destroyAllWindows()